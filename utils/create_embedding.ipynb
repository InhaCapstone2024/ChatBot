{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KoBERT를 이용하여 판결문 데이터를 벡터 임베딩으로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laz2berry\\AppData\\Local\\anaconda3\\envs\\capstone\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\laz2berry\\AppData\\Local\\anaconda3\\envs\\capstone\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\laz2berry\\.cache\\huggingface\\hub\\models--monologg--kobert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 2760/2760 [01:38<00:00, 27.90it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1379/1379 [00:50<00:00, 27.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# KoBERT Github 이용하여 텍스트 데이터 임베딩 생성\n",
    "import json\n",
    "import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Data 불러오기\n",
    "emb_dict = {}\n",
    "data_dict = json.load(open('data/trainval.json', 'r', encoding='utf-8'))\n",
    "\n",
    "# KoBERT 호출\n",
    "model_name = 'monologg/kobert'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "for domain in ['train', 'validation']:\n",
    "    emb_dict[domain] = []\n",
    "\n",
    "    for data in tqdm.tqdm(data_dict[domain]):\n",
    "        first_party = data['The first party']\n",
    "        second_party = data['The second party']\n",
    "        facts = data['facts'].replace('\\n', ' ')\n",
    "        \n",
    "        embeddings = []\n",
    "\n",
    "        for input_data in [first_party, second_party, facts]:\n",
    "            encoded_input = tokenizer([input_data], padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "            with torch.no_grad():\n",
    "                model_output = model(**encoded_input)\n",
    "                embedding = model_output.pooler_output[0].cpu().detach().numpy()  # 또는 model_output.last_hidden_state.mean(dim=1)로 선택 가능\n",
    "        \n",
    "            embeddings.append(embedding)\n",
    "\n",
    "        emb_dict[domain].append(\n",
    "            {\n",
    "                'first_party': embeddings[0],\n",
    "                'first_party_name': first_party,\n",
    "\n",
    "                'second_party': embeddings[1],\n",
    "                'second_party_name': second_party,\n",
    "\n",
    "                'facts': embeddings[2],\n",
    "                'output': data['output']\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # 임베딩 저장\n",
    "    pickle.dump(emb_dict, open('embeddings/trainval.json'.replace('.json', f'_KoBERT.pkl'), 'wb'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1035/1035 [00:35<00:00, 28.95it/s]\n"
     ]
    }
   ],
   "source": [
    "emb_dict = []\n",
    "data_dict = json.load(open('./data/test.json', 'r', encoding='utf-8'))\n",
    "\n",
    "for data in tqdm.tqdm(data_dict):\n",
    "    first_party = data['The first party']\n",
    "    second_party = data['The second party']\n",
    "    facts = data['facts'].replace('\\n', ' ')\n",
    "    \n",
    "    embeddings = []\n",
    "\n",
    "    for input_data in [first_party, second_party, facts]:\n",
    "        encoded_input = tokenizer([input_data], padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "            embedding = model_output.pooler_output[0].cpu().detach().numpy()  # 또는 model_output.last_hidden_state.mean(dim=1)로 선택 가능\n",
    "    \n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    emb_dict.append(\n",
    "        {\n",
    "            'test_id': data['test_id'],\n",
    "\n",
    "            'first_party': embeddings[0],\n",
    "            'first_party_name': first_party,\n",
    "\n",
    "            'second_party': embeddings[1],\n",
    "            'second_party_name': second_party,\n",
    "\n",
    "            'facts': embeddings[2],\n",
    "            'output': data['output']\n",
    "        }\n",
    "    )\n",
    "\n",
    "pickle.dump(emb_dict, open('./embeddings/test.json'.replace('.json', f'_KoBERT.pkl'), 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
